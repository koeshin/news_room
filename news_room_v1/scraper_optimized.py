"""
ìµœì í™”ëœ Playwright ìŠ¤í¬ëž˜í¼
- ë¸Œë¼ìš°ì € ìž¬ì‚¬ìš©
- ë¦¬ì†ŒìŠ¤ ì™„ì „ ì°¨ë‹¨
- ë³‘ë ¬ ì²˜ë¦¬ ê°•í™”
"""

import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import storage

# ë™ì‹œ ì‹¤í–‰ ì œí•œ (ì¦ê°€)
SEM_LIMIT = 15

# ì°¨ë‹¨í•  ë¦¬ì†ŒìŠ¤ íƒ€ìž… (ëª¨ë“  ë¶ˆí•„ìš” ë¦¬ì†ŒìŠ¤)
BLOCKED_RESOURCES = [
    "image", "media", "font", "stylesheet", "script",
    "fetch", "xhr", "websocket", "manifest", "other"
]

async def fetch_article_subtitle_fast(page, url, sem):
    """ë¶€ì œëª©ì„ ë¹ ë¥´ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤. (íŽ˜ì´ì§€ ìž¬ì‚¬ìš©)"""
    async with sem:
        try:
            await page.goto(url, wait_until="domcontentloaded", timeout=3000)
            
            content = await page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            subtitle = ""
            # 1. Standard Subtitle
            sub_elem = soup.select_one('div.media_end_head_subheadline')
            if sub_elem:
                subtitle = sub_elem.get_text(strip=True)
                
            # 2. Summary
            if not subtitle:
                summary_elem = soup.select_one('strong.media_end_summary')
                if summary_elem:
                    subtitle = summary_elem.get_text(strip=True)
                    
                if not subtitle:
                    summary_div = soup.select_one('div.media_end_summary')
                    if summary_div:
                        subtitle = summary_div.get_text(strip=True)
                    
            # 3. Guide
            if not subtitle:
                guide_elem = soup.select_one('div.media_end_head_guide')
                if guide_elem:
                    subtitle = guide_elem.get_text(strip=True)
            
            return subtitle
        except Exception:
            return ""

async def get_newspaper_data_optimized(browser, oid, date, force_refresh=False):
    """ìµœì í™”ëœ ìŠ¤í¬ëž˜í•‘ (ë¸Œë¼ìš°ì € ìž¬ì‚¬ìš©)"""
    
    # 1. ìºì‹œ í™•ì¸
    if not force_refresh:
        cached_data = storage.load_news_cache(date, oid)
        if cached_data:
            print(f"[{oid}] Cache Hit!")
            return cached_data

    print(f"[{oid}] Optimized Scraping started...")
    url = f"https://media.naver.com/press/{oid}/newspaper?date={date}"
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨)
    context = await browser.new_context(
        user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    )
    
    page = await context.new_page()
    
    # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ì´ë¯¸ì§€, í°íŠ¸, ìŠ¤íƒ€ì¼ì‹œíŠ¸ ë“±)
    await page.route("**/*", lambda route: 
        route.abort() if route.request.resource_type in BLOCKED_RESOURCES 
        else route.continue_()
    )
    
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=10000)
        
        # ì§€ë©´ ë°ì´í„° ëŒ€ê¸° (ì§§ì€ íƒ€ìž„ì•„ì›ƒ)
        try:
            await page.wait_for_selector('div.newspaper_inner', timeout=5000)
        except:
            await context.close()
            return []
        
        content = await page.content()
        soup = BeautifulSoup(content, 'html.parser')
        
        page_sections = soup.select('div.newspaper_inner')
        
        newspaper_data = []
        subtitle_tasks = []
        article_infos = []
        
        sem = asyncio.Semaphore(SEM_LIMIT)
        
        # ë¶€ì œëª©ìš© íŽ˜ì´ì§€ë“¤ ë¯¸ë¦¬ ìƒì„±
        subtitle_pages = []
        for _ in range(min(SEM_LIMIT, 10)):
            p = await context.new_page()
            await p.route("**/*", lambda route: 
                route.abort() if route.request.resource_type in BLOCKED_RESOURCES 
                else route.continue_()
            )
            subtitle_pages.append(p)
        
        page_idx = 0
        
        for section in page_sections:
            page_name_elem = section.select_one('span.page_notation')
            if not page_name_elem:
                continue
            
            page_name = page_name_elem.get_text(strip=True)
            
            articles = []
            article_elems = section.select('ul.newspaper_article_lst > li > a')
            
            for a in article_elems:
                title_elem = a.select_one('strong')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                article_url = a['href']
                
                article_info = {
                    "page": page_name,
                    "title": title,
                    "url": article_url,
                    "subtitle": ""
                }
                articles.append(article_info)
                article_infos.append(article_info)
                
                # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ íŽ˜ì´ì§€ í• ë‹¹
                assigned_page = subtitle_pages[page_idx % len(subtitle_pages)]
                page_idx += 1
                subtitle_tasks.append(fetch_article_subtitle_fast(assigned_page, article_url, sem))
            
            if articles:
                newspaper_data.append({
                    "page": page_name,
                    "articles": articles
                })
        
        # ë¶€ì œëª© ë³‘ë ¬ ì²˜ë¦¬
        if subtitle_tasks:
            subtitles = await asyncio.gather(*subtitle_tasks)
            for info, subtitle in zip(article_infos, subtitles):
                info["subtitle"] = subtitle
        
        # ì •ë¦¬
        for p in subtitle_pages:
            await p.close()
        await context.close()
        
        # ìºì‹œ ì €ìž¥
        if newspaper_data:
            storage.save_news_cache(date, oid, newspaper_data)
        
        return newspaper_data
        
    except Exception as e:
        print(f"[{oid}] Error: {e}")
        await context.close()
        return []

async def scrape_multiple_media(media_list, date, force_refresh=False):
    """ì—¬ëŸ¬ ì–¸ë¡ ì‚¬ë¥¼ í•œ ë²ˆì— ìŠ¤í¬ëž˜í•‘ (ë¸Œë¼ìš°ì € 1ê°œ ìž¬ì‚¬ìš©)"""
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € í•œ ë²ˆë§Œ ì‹¤í–‰
        browser = await p.chromium.launch(headless=True)
        
        results = {}
        for media in media_list:
            data = await get_newspaper_data_optimized(browser, media['oid'], date, force_refresh)
            results[media['oid']] = data
        
        await browser.close()
        return results

if __name__ == "__main__":
    import time
    from datetime import datetime, timedelta
    
    # í…ŒìŠ¤íŠ¸ ë‚ ì§œ (ìºì‹œê°€ ìžˆëŠ” ë‚ ì§œ ì‚¬ìš©)
    test_date = "20260130"
    
    TEST_MEDIA = [
        {"name": "ì¡°ì„ ì¼ë³´", "oid": "023"},
        {"name": "ì¤‘ì•™ì¼ë³´", "oid": "025"},
        {"name": "ë™ì•„ì¼ë³´", "oid": "020"},
        {"name": "í•œê²¨ë ˆ", "oid": "028"},
        {"name": "ê²½í–¥ì‹ ë¬¸", "oid": "032"},
    ]
    
    print("="*60)
    print("ðŸš€ ìµœì í™”ëœ Playwright ìŠ¤í¬ëž˜í¼ í…ŒìŠ¤íŠ¸")
    print(f"ðŸ“… ë‚ ì§œ: {test_date}")
    print("="*60)
    
    start = time.time()
    results = asyncio.run(scrape_multiple_media(TEST_MEDIA, test_date, force_refresh=True))
    elapsed = time.time() - start
    
    print("\n" + "="*60)
    print("ðŸ“Š ê²°ê³¼")
    print("="*60)
    
    total_articles = 0
    for oid, data in results.items():
        name = next(m['name'] for m in TEST_MEDIA if m['oid'] == oid)
        article_count = sum(len(page['articles']) for page in data) if data else 0
        total_articles += article_count
        print(f"  {name}: {article_count}ê°œ ê¸°ì‚¬")
    
    print(f"\nâ±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"ðŸ“° ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}ê°œ")
    print(f"âš¡ ê¸°ì‚¬ë‹¹ í‰ê· : {elapsed/total_articles*1000:.1f}ms" if total_articles > 0 else "")
