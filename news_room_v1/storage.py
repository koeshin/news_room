import json
import os
from datetime import datetime

SCRAPS_FILE = "scraps.json"
SETTINGS_FILE = "settings.json"
FOLDERS_FILE = "folders.json"
CACHE_DIR = "scraped_data"

if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR)


DEFAULT_SETTINGS = {
    "media_list": [
        {"name": "ì¡°ì„ ì¼ë³´", "oid": "023"},
        {"name": "ì¤‘ì•™ì¼ë³´", "oid": "025"},
        {"name": "ë™ì•„ì¼ë³´", "oid": "020"}
    ]
}

def load_json(filename, default):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                return json.load(f)
            except json.JSONDecodeError:
                return default
    return default

def save_json(filename, data):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def load_settings():
    return load_json(SETTINGS_FILE, DEFAULT_SETTINGS)

def save_settings(settings):
    save_json(SETTINGS_FILE, settings)

def load_scraps():
    return load_json(SCRAPS_FILE, {})

def load_folders():
    """í´ë” ëª©ë¡ ë¡œë“œ"""
    return load_json(FOLDERS_FILE, {"folders": ["ê¸°ë³¸"], "default": "ê¸°ë³¸"})

def save_folders(folders_data):
    """í´ë” ëª©ë¡ ì €ì¥"""
    save_json(FOLDERS_FILE, folders_data)

def add_folder(folder_name):
    """ìƒˆ í´ë” ì¶”ê°€"""
    folders_data = load_folders()
    if folder_name not in folders_data["folders"]:
        folders_data["folders"].append(folder_name)
        save_folders(folders_data)
        return True
    return False

def get_folder_list():
    """í´ë” ëª©ë¡ ë°˜í™˜"""
    return load_folders().get("folders", ["ê¸°ë³¸"])

def toggle_scrap(date_str, media_name, article, folder="ê¸°ë³¸", tags=None):
    """
    ìŠ¤í¬ë©ì„ ì¶”ê°€í•˜ê±°ë‚˜ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì œê±°í•©ë‹ˆë‹¤. (Toggle)
    Returns: True if added, False if removed
    """
    if tags is None:
        tags = []
        
    scraps = load_scraps()
    if date_str not in scraps:
        scraps[date_str] = []
    
    # ì¤‘ë³µ í™•ì¸ (URL ê¸°ì¤€)
    existing_index = -1
    for idx, s in enumerate(scraps[date_str]):
        if s['url'] == article['url']:
            existing_index = idx
            break
    
    if existing_index != -1:
        # ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì‚­ì œ (Unscrap)
        scraps[date_str].pop(existing_index)
        if not scraps[date_str]:
            del scraps[date_str]
        save_json(SCRAPS_FILE, scraps)
        return False
    else:
        # ì—†ìœ¼ë©´ ì¶”ê°€ (Scrap)
        scrap_item = article.copy()
        scrap_item['media'] = media_name
        scrap_item['scrapped_at'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        scrap_item['read'] = False
        scrap_item['folder'] = folder  # í´ë” ì¶”ê°€
        scrap_item['tags'] = tags  # íƒœê·¸ ì¶”ê°€
        
        scraps[date_str].append(scrap_item)
        save_json(SCRAPS_FILE, scraps)
        return True

def update_scrap_folder(date_str, url, folder):
    """ìŠ¤í¬ë©ì˜ í´ë” ë³€ê²½"""
    scraps = load_scraps()
    if date_str in scraps:
        for s in scraps[date_str]:
            if s['url'] == url:
                s['folder'] = folder
                save_json(SCRAPS_FILE, scraps)
                return True
    return False

def update_scrap_tags(date_str, url, tags):
    """ìŠ¤í¬ë©ì˜ íƒœê·¸ ë³€ê²½"""
    scraps = load_scraps()
    if date_str in scraps:
        for s in scraps[date_str]:
            if s['url'] == url:
                s['tags'] = tags
                save_json(SCRAPS_FILE, scraps)
                return True
    return False

def get_scraps_by_folder(folder_name):
    """íŠ¹ì • í´ë”ì˜ ìŠ¤í¬ë©ë§Œ ë°˜í™˜"""
    scraps = load_scraps()
    result = {}
    for date_str, items in scraps.items():
        filtered = [s for s in items if s.get('folder', 'ê¸°ë³¸') == folder_name]
        if filtered:
            result[date_str] = filtered
    return result

def export_scraps_to_markdown(scraps_data, filename="export.md"):
    """ìŠ¤í¬ë©ì„ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    lines = ["# ìŠ¤í¬ë© ë‚´ë³´ë‚´ê¸°\n"]
    lines.append(f"ìƒì„±ì¼: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    
    for date_str in sorted(scraps_data.keys(), reverse=True):
        lines.append(f"## ğŸ“… {date_str}\n\n")
        for item in scraps_data[date_str]:
            folder = item.get('folder', 'ê¸°ë³¸')
            tags = item.get('tags', [])
            tag_str = " ".join([f"#{t}" for t in tags]) if tags else ""
            
            lines.append(f"### [{item.get('media', '')}] {item['title']}\n")
            if item.get('subtitle'):
                lines.append(f"> {item['subtitle']}\n")
            lines.append(f"- ğŸ“ í´ë”: {folder}\n")
            if tag_str:
                lines.append(f"- ğŸ·ï¸ íƒœê·¸: {tag_str}\n")
            lines.append(f"- ğŸ”— [ê¸°ì‚¬ ë§í¬]({item['url']})\n")
            lines.append(f"- â° ìŠ¤í¬ë©: {item.get('scrapped_at', '')}\n\n")
    
    with open(filename, "w", encoding="utf-8") as f:
        f.writelines(lines)
    
    return filename

def remove_scrap(date_str, url):
    """íŠ¹ì • ìŠ¤í¬ë© ì‚­ì œ (ëª…ì‹œì )"""
    scraps = load_scraps()
    if date_str in scraps:
        original_len = len(scraps[date_str])
        scraps[date_str] = [s for s in scraps[date_str] if s['url'] != url]
        
        if len(scraps[date_str]) != original_len:
            if not scraps[date_str]:
                del scraps[date_str]
            save_json(SCRAPS_FILE, scraps)
            return True
    return False

def mark_as_read(date_str, url, status=True):
    """ì½ìŒ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    scraps = load_scraps()
    if date_str in scraps:
        for s in scraps[date_str]:
            if s['url'] == url:
                s['read'] = status
                save_json(SCRAPS_FILE, scraps)
                return True
    return False

def get_weekly_scraps():
    """
    ì´ë²ˆ ì£¼ ì›”ìš”ì¼ ~ í˜„ì¬ê¹Œì§€ì˜ ìŠ¤í¬ë© ë°ì´í„°ë¥¼ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
    1. ì˜¤ëŠ˜ì´ ì¼ìš”ì¼(6)ì´ë©´: ì§€ë‚œ ì›”(0) ~ í† (5) ë°ì´í„° ìˆ˜ì§‘
    2. ê·¸ ì™¸ ìš”ì¼ì´ë©´: ì´ë²ˆ ì£¼ ì›”(0) ~ ì˜¤ëŠ˜ê¹Œì§€ ë°ì´í„° ìˆ˜ì§‘
    """
    scraps = load_scraps()
    today = datetime.now()
    weekday = today.weekday() # ì›”=0, ì¼=6
    
    target_dates = []
    
    # ë¦¬í¬íŠ¸ ê¸°ì¤€ì¼ ì„¤ì •
    # ë§Œì•½ ì¼ìš”ì¼(6)ì´ë¼ë©´ 'ì§€ë‚œì£¼ ì›”~í† 'ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•¨ (ìš”ì²­ì‚¬í•­ ë°˜ì˜)
    if weekday == 6:
        days_from_mon = 6 # ì¼(6) - ì›”(0) = 6ì¼ ì „ë¶€í„°
        start_date = today - timedelta(days=6)
        end_date = today - timedelta(days=1) # ì–´ì œ(í† )ê¹Œì§€
    else:
        # ì›”~í† ìš”ì¼ì¸ ê²½ìš°: ì´ë²ˆì£¼ ì›”ìš”ì¼ ~ ì˜¤ëŠ˜
        start_date = today - timedelta(days=weekday)
        end_date = today

    # ë‚ ì§œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    curr = start_date
    while curr <= end_date:
        d_str = curr.strftime("%Y-%m-%d")
        if d_str in scraps:
           for item in scraps[d_str]:
               # ë¦¬í¬íŠ¸ìš© í¬ë§·ìœ¼ë¡œ ë³€í™˜ ì—†ì´ ì›ë³¸ ë°˜í™˜
               # í•„ìš”í•œ ê²½ìš° ë‚ ì§œ ì •ë³´ë„ í¬í•¨í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦
               item_with_date = item.copy()
               item_with_date['date'] = d_str
               target_dates.append(item_with_date)
        curr += timedelta(days=1)
        
    return target_dates

def get_cache_path(date, oid):
    # í´ë” êµ¬ì¡°: scraped_data/{date}/{oid}.json
    date_dir = os.path.join(CACHE_DIR, date)
    if not os.path.exists(date_dir):
        os.makedirs(date_dir)
    return os.path.join(date_dir, f"{oid}.json")

def save_news_cache(date, oid, data):
    """ìŠ¤í¬ë© ê²°ê³¼(ì§€ë©´ ë°ì´í„°)ë¥¼ íŒŒì¼ë¡œ ìºì‹±"""
    path = get_cache_path(date, oid)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def load_news_cache(date, oid):
    """ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°˜í™˜, ì—†ìœ¼ë©´ None"""
    path = get_cache_path(date, oid)
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            return None
    return None

def clear_news_cache(date, oid):
    """íŠ¹ì • ìºì‹œ ì‚­ì œ (ê°•ì œ ìƒˆë¡œê³ ì¹¨ìš©)"""
    path = get_cache_path(date, oid)
    if os.path.exists(path):
        os.remove(path)

